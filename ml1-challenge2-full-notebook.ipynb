{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CSE602: Machine Learning - I","metadata":{"papermill":{"duration":0.088276,"end_time":"2022-04-22T22:38:07.71074","exception":false,"start_time":"2022-04-22T22:38:07.622464","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Challenge 2","metadata":{"papermill":{"duration":0.080539,"end_time":"2022-04-22T22:38:07.872073","exception":false,"start_time":"2022-04-22T22:38:07.791534","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Submitted by:\n#### Name      : Muhammad Amin Ghias\n#### ERP ID    : 25366","metadata":{"papermill":{"duration":0.080541,"end_time":"2022-04-22T22:38:08.037275","exception":false,"start_time":"2022-04-22T22:38:07.956734","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Date : 10th April 2022","metadata":{"papermill":{"duration":0.112317,"end_time":"2022-04-22T22:38:08.232828","exception":false,"start_time":"2022-04-22T22:38:08.120511","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# ML-1 Challenge 2: High Income Group Prediction","metadata":{"papermill":{"duration":0.080249,"end_time":"2022-04-22T22:38:08.39497","exception":false,"start_time":"2022-04-22T22:38:08.314721","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{"papermill":{"duration":0.080322,"end_time":"2022-04-22T22:38:08.556042","exception":false,"start_time":"2022-04-22T22:38:08.47572","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:38:08.718897Z","iopub.status.busy":"2022-04-22T22:38:08.71854Z","iopub.status.idle":"2022-04-22T22:38:08.735865Z","shell.execute_reply":"2022-04-22T22:38:08.735156Z"},"papermill":{"duration":0.101491,"end_time":"2022-04-22T22:38:08.738177","exception":false,"start_time":"2022-04-22T22:38:08.636686","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nimport pandas as pd   \nimport matplotlib.pyplot as plt\nimport time\n\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import train_test_split, RepeatedKFold, GridSearchCV, cross_val_score\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.pipeline import Pipeline\nfrom matplotlib import pyplot\n\nfrom numpy import mean\nimport pandas as pd\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\n\nfrom sklearn.preprocessing import LabelEncoder\n# from mixed_naive_bayes import MixedNB\nimport numpy as np\n\n\nimport pandas as pd   \nimport matplotlib.pyplot as plt\nimport time\n\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB, CategoricalNB, GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import train_test_split, RepeatedKFold, GridSearchCV, cross_val_score\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.pipeline import Pipeline\nfrom matplotlib import pyplot\n\nfrom numpy import mean\nimport pandas as pd\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom sklearn.feature_selection import SequentialFeatureSelector\nimport os\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport numpy as np\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import RepeatedKFold\nfrom numpy import arange\nfrom sklearn.linear_model import LassoCV\nimport warnings \nwarnings.filterwarnings('ignore')\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom numpy import mean\nfrom sklearn.feature_selection import SequentialFeatureSelector\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom numpy import mean\n\n\n\ncv = KFold(n_splits=10, random_state=1, shuffle=True)\nreg = LinearRegression()\n","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:38:08.90569Z","iopub.status.busy":"2022-04-22T22:38:08.904773Z","iopub.status.idle":"2022-04-22T22:38:12.441532Z","shell.execute_reply":"2022-04-22T22:38:12.440607Z"},"papermill":{"duration":3.622439,"end_time":"2022-04-22T22:38:12.443812","exception":false,"start_time":"2022-04-22T22:38:08.821373","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading datasets","metadata":{"papermill":{"duration":0.081669,"end_time":"2022-04-22T22:38:12.607499","exception":false,"start_time":"2022-04-22T22:38:12.52583","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/ml-1-challenge-2-high-income-group-prediction/Training.csv\")\ndf.head()\n","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:38:12.774338Z","iopub.status.busy":"2022-04-22T22:38:12.773888Z","iopub.status.idle":"2022-04-22T22:38:13.050167Z","shell.execute_reply":"2022-04-22T22:38:13.049402Z"},"papermill":{"duration":0.363274,"end_time":"2022-04-22T22:38:13.052131","exception":false,"start_time":"2022-04-22T22:38:12.688857","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:38:13.221455Z","iopub.status.busy":"2022-04-22T22:38:13.221168Z","iopub.status.idle":"2022-04-22T22:38:13.237659Z","shell.execute_reply":"2022-04-22T22:38:13.236967Z"},"papermill":{"duration":0.101609,"end_time":"2022-04-22T22:38:13.239545","exception":false,"start_time":"2022-04-22T22:38:13.137936","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:38:13.406567Z","iopub.status.busy":"2022-04-22T22:38:13.40601Z","iopub.status.idle":"2022-04-22T22:38:13.410686Z","shell.execute_reply":"2022-04-22T22:38:13.410132Z"},"papermill":{"duration":0.089847,"end_time":"2022-04-22T22:38:13.412369","exception":false,"start_time":"2022-04-22T22:38:13.322522","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:38:13.583245Z","iopub.status.busy":"2022-04-22T22:38:13.582712Z","iopub.status.idle":"2022-04-22T22:38:13.610598Z","shell.execute_reply":"2022-04-22T22:38:13.609852Z"},"papermill":{"duration":0.116597,"end_time":"2022-04-22T22:38:13.613542","exception":false,"start_time":"2022-04-22T22:38:13.496945","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:38:13.78167Z","iopub.status.busy":"2022-04-22T22:38:13.781104Z","iopub.status.idle":"2022-04-22T22:38:13.78762Z","shell.execute_reply":"2022-04-22T22:38:13.78676Z"},"papermill":{"duration":0.092273,"end_time":"2022-04-22T22:38:13.789377","exception":false,"start_time":"2022-04-22T22:38:13.697104","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:38:13.960609Z","iopub.status.busy":"2022-04-22T22:38:13.959614Z","iopub.status.idle":"2022-04-22T22:38:14.044936Z","shell.execute_reply":"2022-04-22T22:38:14.044088Z"},"papermill":{"duration":0.173791,"end_time":"2022-04-22T22:38:14.047104","exception":false,"start_time":"2022-04-22T22:38:13.873313","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Education Level\"]","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:38:14.222837Z","iopub.status.busy":"2022-04-22T22:38:14.222419Z","iopub.status.idle":"2022-04-22T22:38:14.228226Z","shell.execute_reply":"2022-04-22T22:38:14.227676Z"},"papermill":{"duration":0.09548,"end_time":"2022-04-22T22:38:14.230097","exception":false,"start_time":"2022-04-22T22:38:14.134617","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"High Income\"].unique()","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:38:14.406797Z","iopub.status.busy":"2022-04-22T22:38:14.406405Z","iopub.status.idle":"2022-04-22T22:38:14.41154Z","shell.execute_reply":"2022-04-22T22:38:14.410849Z"},"papermill":{"duration":0.096351,"end_time":"2022-04-22T22:38:14.413202","exception":false,"start_time":"2022-04-22T22:38:14.316851","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"X4\"].unique()","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:38:14.586736Z","iopub.status.busy":"2022-04-22T22:38:14.58627Z","iopub.status.idle":"2022-04-22T22:38:14.594566Z","shell.execute_reply":"2022-04-22T22:38:14.594068Z"},"papermill":{"duration":0.096377,"end_time":"2022-04-22T22:38:14.596383","exception":false,"start_time":"2022-04-22T22:38:14.500006","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1=df.copy()","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:38:14.768791Z","iopub.status.busy":"2022-04-22T22:38:14.768275Z","iopub.status.idle":"2022-04-22T22:38:14.774265Z","shell.execute_reply":"2022-04-22T22:38:14.773673Z"},"papermill":{"duration":0.094542,"end_time":"2022-04-22T22:38:14.776247","exception":false,"start_time":"2022-04-22T22:38:14.681705","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Changing categorical fields into string","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"\ndf1['WorkClass'] = df1['WorkClass'].astype('str')\n\ndf1['Education Level'] = df1['Education Level'].astype('str')\n\ndf1['Marital Status'] = df1['Marital Status'].astype('str')\n\ndf1['Occupation'] = df1['Occupation'].astype('str')\n\ndf1['Gender'] = df1['Gender'].astype('str')\n\ndf1['Native Country'] = df1['Native Country'].astype('str')\n\n\ndf1['X3'] = df1['X3'].astype('str')\n\n# Deleted row ID from df1\ndf1.drop(['row ID'], axis=1, inplace=True)","metadata":{"execution":{"iopub.execute_input":"2022-04-22T20:45:23.531215Z","iopub.status.busy":"2022-04-22T20:45:23.530604Z","iopub.status.idle":"2022-04-22T20:45:23.870619Z","shell.execute_reply":"2022-04-22T20:45:23.869923Z","shell.execute_reply.started":"2022-04-22T20:45:23.531167Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.dtypes","metadata":{"execution":{"iopub.execute_input":"2022-04-22T20:45:24.543125Z","iopub.status.busy":"2022-04-22T20:45:24.542537Z","iopub.status.idle":"2022-04-22T20:45:24.550083Z","shell.execute_reply":"2022-04-22T20:45:24.549511Z","shell.execute_reply.started":"2022-04-22T20:45:24.543079Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using onehot encoding","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"df_onehot = pd.get_dummies(df1)\ndf_onehot.dtypes","metadata":{"execution":{"iopub.execute_input":"2022-04-22T20:45:26.819284Z","iopub.status.busy":"2022-04-22T20:45:26.818376Z","iopub.status.idle":"2022-04-22T20:45:26.916703Z","shell.execute_reply":"2022-04-22T20:45:26.916032Z","shell.execute_reply.started":"2022-04-22T20:45:26.819221Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_onehot.head()","metadata":{"execution":{"iopub.execute_input":"2022-04-22T20:45:31.250742Z","iopub.status.busy":"2022-04-22T20:45:31.249938Z","iopub.status.idle":"2022-04-22T20:45:31.269152Z","shell.execute_reply":"2022-04-22T20:45:31.268626Z","shell.execute_reply.started":"2022-04-22T20:45:31.250702Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finding correlation matrix","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"cor=df.corr()","metadata":{"execution":{"iopub.execute_input":"2022-04-22T20:45:33.13761Z","iopub.status.busy":"2022-04-22T20:45:33.136864Z","iopub.status.idle":"2022-04-22T20:45:33.1856Z","shell.execute_reply":"2022-04-22T20:45:33.184717Z","shell.execute_reply.started":"2022-04-22T20:45:33.137576Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cor","metadata":{"execution":{"iopub.execute_input":"2022-04-22T20:45:34.471608Z","iopub.status.busy":"2022-04-22T20:45:34.471361Z","iopub.status.idle":"2022-04-22T20:45:34.493601Z","shell.execute_reply":"2022-04-22T20:45:34.492424Z","shell.execute_reply.started":"2022-04-22T20:45:34.471582Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_onehot.shape","metadata":{"execution":{"iopub.execute_input":"2022-04-22T20:45:35.564809Z","iopub.status.busy":"2022-04-22T20:45:35.564509Z","iopub.status.idle":"2022-04-22T20:45:35.571348Z","shell.execute_reply":"2022-04-22T20:45:35.570433Z","shell.execute_reply.started":"2022-04-22T20:45:35.564779Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Forming X and y of training data","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"dfX=df_onehot.copy()\ndfX=dfX.drop(columns=['High Income'])\nprint(dfX.shape)\ndfX.head()\nX=dfX\nX.head()\ny=df_onehot['High Income']\ny.head()","metadata":{"execution":{"iopub.execute_input":"2022-04-22T20:45:37.432495Z","iopub.status.busy":"2022-04-22T20:45:37.431782Z","iopub.status.idle":"2022-04-22T20:45:37.464673Z","shell.execute_reply":"2022-04-22T20:45:37.464111Z","shell.execute_reply.started":"2022-04-22T20:45:37.432457Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### loading testing data","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"df_test=pd.read_csv('/kaggle/input/ml-1-challenge-2-high-income-group-prediction/Testing.csv')\ndf_test.head()","metadata":{"execution":{"iopub.execute_input":"2022-04-22T20:46:14.47121Z","iopub.status.busy":"2022-04-22T20:46:14.470619Z","iopub.status.idle":"2022-04-22T20:46:14.649737Z","shell.execute_reply":"2022-04-22T20:46:14.649044Z","shell.execute_reply.started":"2022-04-22T20:46:14.471175Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_test['WorkClass'] = df_test['WorkClass'].astype('str')\n\ndf_test['Education Level'] = df_test['Education Level'].astype('str')\n\ndf_test['Marital Status'] = df_test['Marital Status'].astype('str')\n\ndf_test['Occupation'] = df_test['Occupation'].astype('str')\n\ndf_test['Gender'] = df_test['Gender'].astype('str')\n\ndf_test['Native Country'] = df_test['Native Country'].astype('str')\n\n\ndf_test['X3'] = df_test['X3'].astype('str')\n\ndf_test.drop(['row ID'], axis=1, inplace=True)","metadata":{"execution":{"iopub.execute_input":"2022-04-22T20:46:16.699245Z","iopub.status.busy":"2022-04-22T20:46:16.69885Z","iopub.status.idle":"2022-04-22T20:46:16.841139Z","shell.execute_reply":"2022-04-22T20:46:16.840246Z","shell.execute_reply.started":"2022-04-22T20:46:16.699216Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_onehot_test = pd.get_dummies(df_test)\ndf_onehot_test.dtypes","metadata":{"execution":{"iopub.execute_input":"2022-04-22T20:46:25.85388Z","iopub.status.busy":"2022-04-22T20:46:25.853312Z","iopub.status.idle":"2022-04-22T20:46:25.903887Z","shell.execute_reply":"2022-04-22T20:46:25.903089Z","shell.execute_reply.started":"2022-04-22T20:46:25.853841Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Forming X for testing data","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"\ndfXt=df_onehot_test.copy()\nprint(dfXt.shape)\ndfXt.head()\n\nXt=dfXt\nXt.head()","metadata":{"execution":{"iopub.execute_input":"2022-04-22T20:46:29.229821Z","iopub.status.busy":"2022-04-22T20:46:29.229325Z","iopub.status.idle":"2022-04-22T20:46:29.253201Z","shell.execute_reply":"2022-04-22T20:46:29.252381Z","shell.execute_reply.started":"2022-04-22T20:46:29.229774Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.execute_input":"2022-04-22T20:46:30.677592Z","iopub.status.busy":"2022-04-22T20:46:30.676893Z","iopub.status.idle":"2022-04-22T20:46:30.682202Z","shell.execute_reply":"2022-04-22T20:46:30.681672Z","shell.execute_reply.started":"2022-04-22T20:46:30.677558Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xt.shape","metadata":{"execution":{"iopub.execute_input":"2022-04-22T20:46:31.950135Z","iopub.status.busy":"2022-04-22T20:46:31.949309Z","iopub.status.idle":"2022-04-22T20:46:31.956125Z","shell.execute_reply":"2022-04-22T20:46:31.955493Z","shell.execute_reply.started":"2022-04-22T20:46:31.950092Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Top 5 best Models","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### First best Model","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"gb400 = GradientBoostingClassifier(max_depth=16,n_estimators=3000,max_features=20,verbose=2)\n\ngb400.fit(X,y)\n\ny_pred_test_df = gb400.predict(Xt)\n\n\nmd_probs = gb400.predict_proba(Xt)\n\nb= md_probs[:,1]\n\npr=pd.DataFrame(columns=['High Income'])\npr.head()\n\nfor i in b:\n    print(i)\n    if i>0.30:\n        pr=pr.append({'High Income':1},ignore_index=True)\n    else:\n        pr=pr.append({'High Income':0},ignore_index=True)\n\nXto=pd.read_csv('/kaggle/input/ml-1-challenge-2-high-income-group-prediction/Testing.csv')\n\nro=Xto['row ID']\ntype(ro)\nrow=ro.values\n\nprd=pd.DataFrame({'row ID' : row, 'High Income': pr['High Income']}, index=None)\nprdb=pd.DataFrame({'row ID' : row, 'High Income': b}, index=None)\n\nprd1=pd.DataFrame({'row ID' : row, 'High Income': y_pred_test_df}, index=None)\n\nprd\nprd.head()\n\nprd.to_csv('o50p.csv',index=False)\nprd1.to_csv('o50y.csv',index=False)\nprdb.to_csv('o50.csv',index=False)\n\n","metadata":{"execution":{"iopub.execute_input":"2022-04-22T20:49:51.613692Z","iopub.status.busy":"2022-04-22T20:49:51.613399Z","iopub.status.idle":"2022-04-22T21:07:22.525848Z","shell.execute_reply":"2022-04-22T21:07:22.524517Z","shell.execute_reply.started":"2022-04-22T20:49:51.613664Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2nd Best model","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"gb400 = GradientBoostingClassifier(max_depth=16,n_estimators=3200,max_features=30,verbose=2)\n\ngb400.fit(X,y)\ny_pred_test_df = gb400.predict(Xt)\nmd_probs = gb400.predict_proba(Xt)\nb= md_probs[:,1]\n\npr=pd.DataFrame(columns=['High Income'])\npr.head()\n\nfor i in b:\n    print(i)\n    if i>0.30:\n        pr=pr.append({'High Income':1},ignore_index=True)\n    else:\n        pr=pr.append({'High Income':0},ignore_index=True)\n\nXto=pd.read_csv('/kaggle/input/ml-1-challenge-2-high-income-group-prediction/Testing.csv')\nro=Xto['row ID']\ntype(ro)\nrow=ro.values\n\nprd=pd.DataFrame({'row ID' : row, 'High Income': pr['High Income']}, index=None)\nprdb=pd.DataFrame({'row ID' : row, 'High Income': b}, index=None)\n\nprd1=pd.DataFrame({'row ID' : row, 'High Income': y_pred_test_df}, index=None)\n\nprd\nprd.head()\n\nprd.to_csv('o56p.csv',index=False)\nprd1.to_csv('o56y.csv',index=False)\nprdb.to_csv('o56.csv',index=False)\n\n\n\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3rd best model","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"gb400 = GradientBoostingClassifier(max_depth=16,n_estimators=2700,verbose=2)\n\ngb400.fit(X,y)\n\ny_pred_test_df = gb400.predict(Xt)\n\nmd_probs = gb400.predict_proba(Xt)\nb= md_probs[:,1]\n\npr=pd.DataFrame(columns=['High Income'])\npr.head()\n\nfor i in b:\n    print(i)\n    if i>0.30:\n        pr=pr.append({'High Income':1},ignore_index=True)\n    else:\n        pr=pr.append({'High Income':0},ignore_index=True)\n\nXto=pd.read_csv('/kaggle/input/ml-1-challenge-2-high-income-group-prediction/Testing.csv')\nro=Xto['row ID']\ntype(ro)\nrow=ro.values\n\nprd=pd.DataFrame({'row ID' : row, 'High Income': pr['High Income']}, index=None)\nprdb=pd.DataFrame({'row ID' : row, 'High Income': b}, index=None)\n\nprd1=pd.DataFrame({'row ID' : row, 'High Income': y_pred_test_df}, index=None)\n\nprd\nprd.head()\n\nprd.to_csv('o48p.csv',index=False)\nprd1.to_csv('o48y.csv',index=False)\nprdb.to_csv('o48.csv',index=False)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4th best model","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"gb400 = GradientBoostingClassifier(max_depth=16,n_estimators=2500,verbose=2)\n\ngb400.fit(X,y)\n\n\n\ny_pred_test_df = gb400.predict(Xt)\n\n\nmd_probs = gb400.predict_proba(Xt)\n\nb= md_probs[:,1]\n\npr=pd.DataFrame(columns=['High Income'])\npr.head()\n\nfor i in b:\n    print(i)\n    if i>0.30:\n        pr=pr.append({'High Income':1},ignore_index=True)\n    else:\n        pr=pr.append({'High Income':0},ignore_index=True)\n\n\n\nXto=pd.read_csv('/kaggle/input/ml-1-challenge-2-high-income-group-prediction/Testing.csv')\nro=Xto['row ID']\ntype(ro)\nrow=ro.values\n\nprd=pd.DataFrame({'row ID' : row, 'High Income': pr['High Income']}, index=None)\nprdb=pd.DataFrame({'row ID' : row, 'High Income': b}, index=None)\n\nprd1=pd.DataFrame({'row ID' : row, 'High Income': y_pred_test_df}, index=None)\n\nprd\nprd.head()\n\nprd.to_csv('o41p.csv',index=False)\nprd1.to_csv('o41y.csv',index=False)\nprdb.to_csv('o41.csv',index=False)\n\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5th best model","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"gb400 = GradientBoostingClassifier(max_depth=16,n_estimators=2000,verbose=2)\n\ngb400.fit(X,y)\n\ny_pred_test_df = gb400.predict(Xt)\n\nmd_probs = gb400.predict_proba(Xt)\n\nb= md_probs[:,1]\n\npr=pd.DataFrame(columns=['High Income'])\npr.head()\n\nfor i in b:\n    print(i)\n    if i>0.30:\n        pr=pr.append({'High Income':1},ignore_index=True)\n    else:\n        pr=pr.append({'High Income':0},ignore_index=True)\n\n\nXto=pd.read_csv('/kaggle/input/ml-1-challenge-2-high-income-group-prediction/Testing.csv')\nro=Xto['row ID']\ntype(ro)\nrow=ro.values\n\nprd=pd.DataFrame({'row ID' : row, 'High Income': pr['High Income']}, index=None)\nprdb=pd.DataFrame({'row ID' : row, 'High Income': b}, index=None)\n\nprd1=pd.DataFrame({'row ID' : row, 'High Income': y_pred_test_df}, index=None)\n\nprd\nprd.head()\n\nprd.to_csv('o46p.csv',index=False)\nprd1.to_csv('o46y.csv',index=False)\nprdb.to_csv('o46.csv',index=False)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Taking mean of our top 5 best models results","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"Xtstack=pd.read_csv('../input/ml-challenge2-kaggle-dataset/xts.csv')\n\nXtst=pd.read_csv('../input/ml-challenge2-kaggle-dataset/xts_stack.csv')","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:04:41.112588Z","iopub.status.busy":"2022-04-22T22:04:41.112065Z","iopub.status.idle":"2022-04-22T22:04:41.227211Z","shell.execute_reply":"2022-04-22T22:04:41.226289Z","shell.execute_reply.started":"2022-04-22T22:04:41.112555Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtst['average'] = Xtst.mean(axis=1)","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:04:57.435203Z","iopub.status.busy":"2022-04-22T22:04:57.434728Z","iopub.status.idle":"2022-04-22T22:04:57.441Z","shell.execute_reply":"2022-04-22T22:04:57.440318Z","shell.execute_reply.started":"2022-04-22T22:04:57.435169Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtst.head()","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:04:58.50062Z","iopub.status.busy":"2022-04-22T22:04:58.500357Z","iopub.status.idle":"2022-04-22T22:04:58.512536Z","shell.execute_reply":"2022-04-22T22:04:58.511975Z","shell.execute_reply.started":"2022-04-22T22:04:58.500592Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xto=pd.read_csv('../input/ml-challenge2-kaggle-dataset/xts.csv')\nro=Xto['row ID']\ntype(ro)\nrow=ro.values\n\nprd=pd.DataFrame({'row ID' : row, 'High Income': Xtst['average']}, index=None)\n\n\nprd\nprd.head()\n\nprd.to_csv('o63.csv',index=False)\n\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hc=pd.read_csv('../input/ml-challenge2-kaggle-dataset/o50 - Copy.csv')\nhc.head()\nhc1=hc.drop(columns=['High Income1','High Income2','High Income3','High Income4','High Income5'])\nhc1.head()\nhc1.to_csv('oavg.csv',index=False)","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:06:47.181875Z","iopub.status.busy":"2022-04-22T22:06:47.181518Z","iopub.status.idle":"2022-04-22T22:06:47.305361Z","shell.execute_reply":"2022-04-22T22:06:47.304585Z","shell.execute_reply.started":"2022-04-22T22:06:47.181842Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Background Work and Analysis","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"## Filtering Methods","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"#  Removing of self correlation\n\n# This  function will find correlation feateures with each other and tell feaure with are highly correlataed nearly same which can be removed\n\ndef self_corr(cormatrix,low_range,high_range):\n\n    cor=cormatrix\n\n\n    fcr_df=pd.DataFrame(columns= ['feature 1', 'feature 2', 'correlation values'])\n    l=len(cor)\n    fcor_rem=[]\n    l\n    for j in range(l):\n        for k in range(l):\n            fcor=cor.iloc[j-1,k-1]\n\n            if fcor>=low_range and fcor <high_range:\n\n                \n                f2=cor.index[j-1]\n                f3=cor.columns[k-1]\n\n                \n                fcor_rem.append([f2, f3, fcor])\n                fcr_df=fcr_df.append({'feature 1': f2, 'feature 2' : f3, 'correlation values': fcor}, ignore_index=True)\n                \n\n\n    fcr12=fcr_df.copy()\n\n    for i in range(len(fcr12)):\n        for j in range(len(fcr12)):\n            try:\n\n                if fcr12.iloc[j,0]==fcr12.iloc[i,1]:\n                    fcr12=fcr12.drop(j)\n            except:\n                    \n\n             fcr12 =fcr12.reset_index(drop=True)\n\n    print(fcr_df.head())\n    return(fcr12['feature 1'])\n        \n\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Removing of features on correlation with target variable\n\n# if feature has very low correlation with target variable it can be neglected\n\n\ndef target_cor(cor_matrix,target_feature,lower_limit):\n\n    cor= cor_matrix\n\n\n    cor_target = abs(cor[target_feature])\n\n    #Selecting highly correlated features\n    relevant_features = cor_target[cor_target<lower_limit]\n    print(relevant_features)\n    return(relevant_features.index)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Removing features with low self variance\n\n## Features whcih remain constant can have no effect on predicting targert feature and can be removed\n\ndef self_var(dataset,limit):\n\n    var=dataset.var()\n    var_rem= var[var<limit]\n    print(var_rem)\n    return(var_rem.index)\n\n    ","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_features(dataset,rem_feature):\n\n    for i in rem_feature:\n        try:\n            dataset=dataset.drop(columns= i)\n            dataset=dataset.reset_index(drop=True)\n\n            print(dataset.shape)\n\n        except:\n            print('Missing')\n\n    return(dataset)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This will remove features based on higher p-values\n\ndef p_val(dataset):\n\n    p= len(dataset.columns)\n    n= len(dataset.index)\n\n    print(dataset.shape)\n    Xsm=dataset.copy()\n    Xsm = sm.add_constant(dataset)\n    print(dataset.shape)\n    mod = sm.OLS(y, Xsm)\n    res = mod.fit()\n    # # print(res.summary())\n\n    ar=pd.DataFrame(columns = ['Features', 'P_Values'])\n    c=[]\n    for l in Xsm.columns:\n        b = res.pvalues[l]\n        if b > 0.05:\n\n            ar= ar.append({'Features': l, 'P_Values': b},ignore_index=True)\n            c.append(l)\n            print(l,':',b)\n    print(c)\n    # ar.head(30)\n\n    return(c)\n\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scaling(dataset):\n    from sklearn.preprocessing import MinMaxScaler\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    X2_Scaled = scaler.fit_transform(dataset)\n    X2df_Scaled = pd.DataFrame(X2_Scaled)\n    X2df_Scaled.columns = dataset.columns\n    return(X2df_Scaled)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nrem_selfcor =self_corr(cor,0.9,1)\n\nrem_cor=target_cor(cor,'High Income',0.1)\n\nrem_var=self_var(X,0.1)\n\nrem_p=p_val(X)\n\nX2=remove_features(X,rem_cor)\n\nX2=remove_features(X2,rem_selfcor)\n\nX2=remove_features(X2,rem_var)\n\nX2=remove_features(X2,rem_p)\n\nX2t=Xt[X2.columns]\n\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X2.shape","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying different models on train data with all features using train_test split","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.3, random_state=2)\n\ndef fit_model(model, model_name):\n    model.fit(trainX,trainy)\n    md_probs = model.predict_proba(testX)\n    md_probs = md_probs[:,1]\n    md_auc = roc_auc_score(testy, md_probs)\n    print(model_name, \" : \", md_auc)\n    md_fpr, md_tpr, _ = roc_curve(testy, md_probs)\n    pyplot.plot(md_fpr, md_tpr, marker='.', label=model_name)\n    #return (md_fpr, md_tpr)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression()\nfit_model(lr, \"Logistic\")\n\ndt = DecisionTreeClassifier(max_depth=5)  \nfit_model(dt, \"Decision Tree\") \n\nrf = RandomForestClassifier(max_depth=5,n_estimators=50)\nfit_model(rf, \"Random Forest\")\n\ngb = GradientBoostingClassifier(max_depth=2,n_estimators=50)\nfit_model(gb, \"Graident Boosting\")\n\nkn = KNeighborsClassifier(n_neighbors=5)\nfit_model(kn, \"k-NN\")\n\npipe_kn = Pipeline([(\"scaler\", MinMaxScaler()), (\"knr\", KNeighborsClassifier(n_neighbors=5))])\nfit_model(pipe_kn, \"Scaled k-NN\")\n\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## results with varried parameters","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"lr = LogisticRegression()\nfit_model(lr, \"Logistic\")\n\ndt = DecisionTreeClassifier(max_depth=5)  \nfit_model(dt, \"Decision Tree\") \n\nrf = RandomForestClassifier(verbose=2, max_depth=5,n_estimators=300)\nfit_model(rf, \"Random Forest\")\n\ngb = GradientBoostingClassifier(verbose=2,max_depth=12,n_estimators=1000)\nfit_model(gb, \"Graident Boosting\")\n\nkn = KNeighborsClassifier(n_neighbors=5)\nfit_model(kn, \"k-NN\")\n\npipe_kn = Pipeline([(\"scaler\", MinMaxScaler()), (\"knr\", KNeighborsClassifier(n_neighbors=5))])\nfit_model(pipe_kn, \"Scaled k-NN\")\n\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finding Important features with Random Forrest Classifier","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"clfRF = RandomForestClassifier(max_depth=5, max_features=4, min_samples_split=8,\n                      n_estimators=100)#, random_state=0)\nclfRF.fit(trainX, trainy)\nimportance_rf = pd.Series(clfRF.feature_importances_, index=trainX.columns)\nimportance_rf_sorted = importance_rf.sort_values()\nimportance_rf_sorted.nlargest(20).plot(kind='barh', color='orange')\nplt.title(\"Feature Importance Random Forest\")\nplt.show()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn\nsklearn.metrics.SCORERS.keys()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fitting_models_CV2():\n    cv = RepeatedKFold(n_splits=10, n_repeats=1)#, random_state=1)\n    \n    lr=LogisticRegression()\n    dt = DecisionTreeClassifier(max_depth=5)    \n    rf = RandomForestClassifier(max_depth=5,n_estimators=50)\n    gb = GradientBoostingClassifier(max_depth=2,n_estimators=50)\n    kn = KNeighborsClassifier(n_neighbors=5)\n    pipe_kn = Pipeline([(\"scaler\", MinMaxScaler()), \n                         (\"knr\", KNeighborsClassifier(n_neighbors=5))])\n    clfs = [('Logistic Regression', lr),        \n        ('Decision Tree', dt),        \n        ('Random Forest', rf),\n        ('Gradient Boosting', gb),\n        ('KNearest Neighbor',kn),\n        ('Scaled KNearest',pipe_kn)       \n    ]\n    for name,clf in clfs:\n        start = time.perf_counter()\n        #scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=cv) \n        scores = cross_val_score(clf, X, y, scoring=\"roc_auc\", cv=cv) \n        end = time.perf_counter()        \n        score = format(mean(scores), '.4f')\n        duration = format((end-start),'.4f')\n        print(\"{} : {} - {}\".format(name,score,duration))","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trying to find best parameters of gradient boosting","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"n= [1000, 1200, 1500]\n\nfor i in n:\n    print(i)\n\n    trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.3, random_state=2)\n\n    gb = GradientBoostingClassifier(verbose=2, max_depth=12,n_estimators=i)\n\n    gb.fit(trainX,trainy)\n    md_probs = gb.predict_proba(testX)\n    md_probs = md_probs[:,1]\n    md_auc = roc_auc_score(testy, md_probs)\n    print(\"Graident Boosting, max_depth: \" , 16 ,\" : \", md_auc)\n    md_fpr, md_tpr, _ = roc_curve(testy, md_probs)\n\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gradient boosting","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"gb400 = GradientBoostingClassifier(max_depth=20,n_estimators=2500,verbose=2)\n\ngb400.fit(X,y)\n\n\n\ny_pred_test_df = gb400.predict(Xt)\n\n\nXto=pd.read_csv('/kaggle/input/ml-1-challenge-2-high-income-group-prediction/Testing.csv')\nro=Xto['row ID']\ntype(ro)\nrow=ro.values\n\nprd=pd.DataFrame({'row ID' : row, 'High Income': y_pred_test_df}, index=None)\n\nprd\nprd.head()\n\nprd.to_csv('o29.csv',index=False)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decission Tree","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"gb400 = dt = DecisionTreeClassifier(max_depth=25)\n\ngb400.fit(X,y)\n\n\n\ny_pred_test_df = gb400.predict(Xt)\n\n\nXto=pd.read_csv('/kaggle/input/ml-1-challenge-2-high-income-group-prediction/Testing.csv')\nro=Xto['row ID']\ntype(ro)\nrow=ro.values\n\nprd=pd.DataFrame({'row ID' : row, 'High Income': y_pred_test_df}, index=None)\n\nprd\nprd.head()\n\nprd.to_csv('o30.csv',index=False)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Forming a dataframe to store results of AUC socre on train data with different parameters of Decission Tree Classifier","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"results=pd.DataFrame(columns=['Name','depth','estimators','AUC'])\n\nresults.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrainX, testX, trainy, testy = train_test_split(X, y, test_size=0.3, random_state=2)\nname= \"Decission Tree\"\nn='na'\ni= range(1,100)\n\n\nfor d in i:\n    print(d)\n\n    gb = dt = DecisionTreeClassifier(max_depth=d)\n\n    gb.fit(trainX,trainy)\n    md_probs = gb.predict_proba(testX)\n    md_probs = md_probs[:,1]\n    md_auc = roc_auc_score(testy, md_probs)\n    print(\"Decission Tree \", md_auc)\n    s=md_auc\n    md_fpr, md_tpr, _ = roc_curve(testy, md_probs)\n    results=results.append({'Name':name,\n                   'depth': d,'estimators': n,'AUC':s}, ignore_index=True) \n\nresults.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Forming a dataframe to store results of AUC socre on train data with different parameters of Random Classifier","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"results2=pd.DataFrame(columns=['Name','depth','estimators','AUC'])\n\nresults2.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nname= \"RandomForestClassifier\"\nn=100\ni= range(1,100)\n\nfor d in i:\n    print(d)\n    gb = dt = RandomForestClassifier(max_depth=d,n_estimators=n)\n    gb.fit(trainX,trainy)\n    md_probs = gb.predict_proba(testX)\n    md_probs = md_probs[:,1]\n    md_auc = roc_auc_score(testy, md_probs)\n    print(\"Random Forrest Classifier\", md_auc)\n    s=md_auc\n    md_fpr, md_tpr, _ = roc_curve(testy, md_probs)\n    results2=results2.append({'Name':name,\n                   'depth': d,'estimators': n,'AUC':s}, ignore_index=True) \n\nresults2.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results2.sort_values(by=['AUC'],ascending=False)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c= [200, 400, 600, 800, 1000]\nname= \"RandomForestClassifier\"\nn=100\nd=30\n\n\nfor n in c:\n    print(n)\n\n    gb = dt = RandomForestClassifier(verbose=2, max_depth=d,n_estimators=n)\n    gb.fit(trainX,trainy)\n    md_probs = gb.predict_proba(testX)\n    md_probs = md_probs[:,1]\n    md_auc = roc_auc_score(testy, md_probs)\n    print(\"Random Forrest Classifier\", md_auc)\n    s=md_auc\n    md_fpr, md_tpr, _ = roc_curve(testy, md_probs)\n    results2=results2.append({'Name':name,\n                   'depth': d,'estimators': n,'AUC':s}, ignore_index=True) \n\nresults2.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results2.tail()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results2.sort_values(by=['AUC'],ascending=False)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c= [1200, 1500, 1800, 2200, 2600, 3000]\n# trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.3, random_state=2)\nname= \"RandomForestClassifier\"\n# n=100\n# i= range(1,100)\nd='na'\n\n\nfor n in c:\n    print(n)\n\n    # trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.3, random_state=2)\n\n    gb = dt = RandomForestClassifier(verbose=2, n_estimators=n)\n\n        # fit_model(gb, \"Graident Boosting\")\n\n    gb.fit(trainX,trainy)\n    md_probs = gb.predict_proba(testX)\n    md_probs = md_probs[:,1]\n    md_auc = roc_auc_score(testy, md_probs)\n    print(\"Graident Boosting, max_depth: : \", md_auc)\n    s=md_auc\n    md_fpr, md_tpr, _ = roc_curve(testy, md_probs)\n    results2=results2.append({'Name':name,\n                   'depth': d,'estimators': n,'AUC':s}, ignore_index=True) \n\n# results2.head()\n\n\nresults2.sort_values(by=['AUC'],ascending=False)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying scaling on dataset to see its effect on results","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"Xs=scaling(X)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xs.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xts=scaling(Xt)\nXts.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c= [1000]\n\nname= \"Scaled-RandomForestClassifier\"\nd=30\n\nfor n in c:\n    print(n)\n\n    gb = dt = RandomForestClassifier(verbose=2, n_estimators=n)\n    gb.fit(trainX,trainy)\n    md_probs = gb.predict_proba(testX)\n    md_probs = md_probs[:,1]\n    md_auc = roc_auc_score(testy, md_probs)\n    print(\"Random Forrest Classifier\", md_auc)\n    s=md_auc\n    md_fpr, md_tpr, _ = roc_curve(testy, md_probs)\n    results2=results2.append({'Name':name,\n                   'depth': d,'estimators': n,'AUC':s}, ignore_index=True) \n\nresults2.sort_values(by=['AUC'],ascending=False)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c= [500, 1500, 2000, 2500]\ntrainX, testX, trainy, testy = train_test_split(Xs, y, test_size=0.3, random_state=2)\nname= \"Scaled-RandomForestClassifier\"\n\nd=30\n\n\nfor n in c:\n    print(n)\n    gb = dt = RandomForestClassifier(verbose=2, n_estimators=n, max_depth=d)\n\n    gb.fit(trainX,trainy)\n    md_probs = gb.predict_proba(testX)\n    md_probs = md_probs[:,1]\n    md_auc = roc_auc_score(testy, md_probs)\n    print(\"Random Forrest Classifier \", md_auc)\n    s=md_auc\n    md_fpr, md_tpr, _ = roc_curve(testy, md_probs)\n    results2=results2.append({'Name':name,\n                   'depth': d,'estimators': n,'AUC':s}, ignore_index=True) \n                   \nresults2.sort_values(by=['AUC'],ascending=False)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results2.sort_values(by=['AUC'],ascending=False)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## AdaBoostClassifier","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ngb400 = AdaBoostClassifier(RandomForestClassifier(verbose=2, n_estimators=200, max_depth=30, max_features=20))\n\ngb400.fit(X,y)\n\ny_pred_test_df = gb400.predict(Xt)\n\n\nmd_probs = gb400.predict_proba(Xt)\n\nb= md_probs[:,1]\n\npr=pd.DataFrame(columns=['High Income'])\npr.head()\n\nfor i in b:\n    print(i)\n    if i>0.30:\n        pr=pr.append({'High Income':1},ignore_index=True)\n    else:\n        pr=pr.append({'High Income':0},ignore_index=True)\n\nXto=pd.read_csv('/kaggle/input/ml-1-challenge-2-high-income-group-prediction/Testing.csv')\nro=Xto['row ID']\ntype(ro)\nrow=ro.values\n\nprd=pd.DataFrame({'row ID' : row, 'High Income': pr['High Income']}, index=None)\nprdb=pd.DataFrame({'row ID' : row, 'High Income': b}, index=None)\n\nprd1=pd.DataFrame({'row ID' : row, 'High Income': y_pred_test_df}, index=None)\n\nprd\nprd.head()\n\nprd.to_csv('o43p.csv',index=False)\nprd1.to_csv('o43y.csv',index=False)\nprdb.to_csv('o43.csv',index=False)\n\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Doing analysis on top 70 features from random forrest calssifier selection","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"importance_rf_sorted","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance_rf_sorted = importance_rf.sort_values(ascending = False)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance_rf_sorted ","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance_rf_sorted.index","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r=range(0,71)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col=[]\n\nfor i in r:\n    print(i)\n    col.append(importance_rf_sorted.index[i])","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xsel=X[col]","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xsel.shape","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xselt=Xt[col]","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xselt.shape","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Findinng result of sleceted features datased and comparing with full dataset results","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"c= [2000]\ntrainX, testX, trainy, testy = train_test_split(Xsel, y, test_size=0.3, random_state=2)\nname= \" RandomForestClassifier max_features=20\"\n\nfor n in c:\n    print(n)\n    gb = RandomForestClassifier(verbose=2, n_estimators=2000, max_depth=60, max_features=20)\n\n\n    gb.fit(trainX,trainy)\n    md_probs = gb.predict_proba(testX)\n    md_probs = md_probs[:,1]\n    md_auc = roc_auc_score(testy, md_probs)\n    print(\"Random Forrest Classifier\", md_auc)\n    s=md_auc\n    md_fpr, md_tpr, _ = roc_curve(testy, md_probs)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results are better on full dataset","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"markdown","source":"## KNN Classifier","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"\n\npipe_kn = Pipeline([(\"scaler\", MinMaxScaler()), (\"knr\", KNeighborsClassifier(n_neighbors=10))])\n\ngb400 = pipe_kn\n\ngb400.fit(X,y)\n\n\n\ny_pred_test_df = gb400.predict(Xt)\n\n\nmd_probs = gb400.predict_proba(Xt)\n\nb= md_probs[:,1]\n\npr=pd.DataFrame(columns=['High Income'])\npr.head()\n\nfor i in b:\n    print(i)\n    if i>0.30:\n        pr=pr.append({'High Income':1},ignore_index=True)\n    else:\n        pr=pr.append({'High Income':0},ignore_index=True)\n\n\n\nXto=pd.read_csv('/kaggle/input/ml-1-challenge-2-high-income-group-prediction/Testing.csv')\nro=Xto['row ID']\ntype(ro)\nrow=ro.values\n\nprd=pd.DataFrame({'row ID' : row, 'High Income': pr['High Income']}, index=None)\nprdb=pd.DataFrame({'row ID' : row, 'High Income': b}, index=None)\n\nprd1=pd.DataFrame({'row ID' : row, 'High Income': y_pred_test_df}, index=None)\n\nprd\nprd.head()\n\nprd.to_csv('o54p.csv',index=False)\nprd1.to_csv('o54y.csv',index=False)\nprdb.to_csv('o54.csv',index=False)\n\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"\ngb400 = LogisticRegression()\n\ngb400.fit(X,y)\n\ny_pred_test_df = gb400.predict(Xt)\n\n\nmd_probs = gb400.predict_proba(Xt)\n\nb= md_probs[:,1]\n\npr=pd.DataFrame(columns=['High Income'])\npr.head()\n\nfor i in b:\n    print(i)\n    if i>0.30:\n        pr=pr.append({'High Income':1},ignore_index=True)\n    else:\n        pr=pr.append({'High Income':0},ignore_index=True)\n\nXto=pd.read_csv('/kaggle/input/ml-1-challenge-2-high-income-group-prediction/Testing.csv')\nro=Xto['row ID']\ntype(ro)\nrow=ro.values\n\nprd=pd.DataFrame({'row ID' : row, 'High Income': pr['High Income']}, index=None)\nprdb=pd.DataFrame({'row ID' : row, 'High Income': b}, index=None)\n\nprd1=pd.DataFrame({'row ID' : row, 'High Income': y_pred_test_df}, index=None)\n\nprd\nprd.head()\n\nprd.to_csv('o51p.csv',index=False)\nprd1.to_csv('o51y.csv',index=False)\nprdb.to_csv('o51.csv',index=False)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gaussian NB","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"\ngb400 = nb_g = GaussianNB()\n\ngb400.fit(X,y)\n\ny_pred_test_df = gb400.predict(Xt)\n\nmd_probs = gb400.predict_proba(Xt)\n\nb= md_probs[:,1]\n\npr=pd.DataFrame(columns=['High Income'])\npr.head()\n\nfor i in b:\n    print(i)\n    if i>0.30:\n        pr=pr.append({'High Income':1},ignore_index=True)\n    else:\n        pr=pr.append({'High Income':0},ignore_index=True)\n\n\nXto=pd.read_csv('/kaggle/input/ml-1-challenge-2-high-income-group-prediction/Testing.csv')\nro=Xto['row ID']\ntype(ro)\nrow=ro.values\n\nprd=pd.DataFrame({'row ID' : row, 'High Income': pr['High Income']}, index=None)\nprdb=pd.DataFrame({'row ID' : row, 'High Income': b}, index=None)\n\nprd1=pd.DataFrame({'row ID' : row, 'High Income': y_pred_test_df}, index=None)\n\nprd\nprd.head()\n\nprd.to_csv('o53p.csv',index=False)\nprd1.to_csv('o53y.csv',index=False)\nprdb.to_csv('o53.csv',index=False)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacking","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"\nfrom sklearn.ensemble import StackingClassifier\n\nestimators = [\n('cart' , DecisionTreeClassifier(max_depth=25)),\n('rf', RandomForestClassifier(verbose=2, n_estimators=1000, max_depth=30, max_features=20)),\n('gb',GradientBoostingClassifier(max_depth=20,n_estimators=2000, verbose=2))\n]\n\nreg_sr = StackingClassifier(verbose=2, cv=2, estimators=estimators, final_estimator=RandomForestClassifier(max_depth=30, n_estimators=1000,random_state=42, verbose=10))\n\nreg_sr.fit(X,y)\ny_pred_test_df = reg_sr.predict(Xt)\nmd_probs = reg_sr.predict_proba(Xt)\n\nb= md_probs[:,1]\n\npr=pd.DataFrame(columns=['High Income'])\npr.head()\n\nfor i in b:\n    print(i)\n    if i>0.60:\n        pr=pr.append({'High Income':1},ignore_index=True)\n    else:\n        pr=pr.append({'High Income':0},ignore_index=True)\n\n\nXto=pd.read_csv('/kaggle/input/ml-1-challenge-2-high-income-group-prediction/Testing.csv')\nro=Xto['row ID']\ntype(ro)\nrow=ro.values\n\nprd=pd.DataFrame({'row ID' : row, 'High Income': pr['High Income']}, index=None)\nprdb=pd.DataFrame({'row ID' : row, 'High Income': b}, index=None)\n\nprd1=pd.DataFrame({'row ID' : row, 'High Income': y_pred_test_df}, index=None)\n\nprd\nprd.head()\n\nprd.to_csv('o40p.csv',index=False)\nprd1.to_csv('o40y.csv',index=False)\nprdb.to_csv('o40.csv',index=False)\n\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.ensemble import StackingClassifier\n\nestimators = [\n('rf', RandomForestClassifier(verbose=2, n_estimators=1000, max_depth=50, max_features=20)),\n('gb',GradientBoostingClassifier(max_depth=16,n_estimators=3000,max_features=20,verbose=2))\n]\n\nreg_sr = StackingClassifier(verbose=2, cv=2, estimators=estimators, final_estimator=RandomForestClassifier(max_depth=50, n_estimators=1000,random_state=42, verbose=10))\n\nreg_sr.fit(X,y)\ny_pred_test_df = reg_sr.predict(Xt)\n\nmd_probs = reg_sr.predict_proba(Xt)\n\nb= md_probs[:,1]\n\npr=pd.DataFrame(columns=['High Income'])\npr.head()\n\nfor i in b:\n    print(i)\n    if i>0.60:\n        pr=pr.append({'High Income':1},ignore_index=True)\n    else:\n        pr=pr.append({'High Income':0},ignore_index=True)\n\nXto=pd.read_csv('/kaggle/input/ml-1-challenge-2-high-income-group-prediction/Testing.csv')\nro=Xto['row ID']\ntype(ro)\nrow=ro.values\n\nprd=pd.DataFrame({'row ID' : row, 'High Income': pr['High Income']}, index=None)\nprdb=pd.DataFrame({'row ID' : row, 'High Income': b}, index=None)\n\nprd1=pd.DataFrame({'row ID' : row, 'High Income': y_pred_test_df}, index=None)\n\nprd\nprd.head()\n\nprd.to_csv('o55p.csv',index=False)\nprd1.to_csv('o55y.csv',index=False)\nprdb.to_csv('o55.csv',index=False)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Doing Stacking manually on 4 best results","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"gb400 = GradientBoostingClassifier(max_depth=16,n_estimators=2000,verbose=2)\n\ngb400.fit(X,y)\n\ny_pred_test_df = gb400.predict(X)\n\n\nmd_probs = gb400.predict_proba(X)\n\nb= md_probs[:,1]\n\npr=pd.DataFrame(columns=['High Income'])\npr.head()\n\nfor i in b:\n    print(i)\n    if i>0.30:\n        pr=pr.append({'High Income':1},ignore_index=True)\n    else:\n        pr=pr.append({'High Income':0},ignore_index=True)\n\n\n\nXto=pd.read_csv('/kaggle/input/ml-1-challenge-2-high-income-group-prediction/Testing.csv')\nro=Xto['row ID']\ntype(ro)\nrow=ro.values\n\nprd=pd.DataFrame({'row ID' : row, 'High Income': pr['High Income']}, index=None)\nprdb=pd.DataFrame({'row ID' : row, 'High Income': b}, index=None)\n\nprd1=pd.DataFrame({'row ID' : row, 'High Income': y_pred_test_df}, index=None)\n\nprd\nprd.head()\n\nprd.to_csv('s5p.csv',index=False)\nprd1.to_csv('s5y.csv',index=False)\nprdb.to_csv('s5.csv',index=False)\n\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtst.shape","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:17:19.745964Z","iopub.status.busy":"2022-04-22T22:17:19.745687Z","iopub.status.idle":"2022-04-22T22:17:19.751351Z","shell.execute_reply":"2022-04-22T22:17:19.750737Z","shell.execute_reply.started":"2022-04-22T22:17:19.745936Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtstack.shape","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:17:21.189938Z","iopub.status.busy":"2022-04-22T22:17:21.189321Z","iopub.status.idle":"2022-04-22T22:17:21.1951Z","shell.execute_reply":"2022-04-22T22:17:21.194334Z","shell.execute_reply.started":"2022-04-22T22:17:21.189903Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The s1, s2, s3, s4 , s5 are result outputs of train datasaets obtained on our top 5 models","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"Xtst.head()","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:17:44.37351Z","iopub.status.busy":"2022-04-22T22:17:44.373082Z","iopub.status.idle":"2022-04-22T22:17:44.386233Z","shell.execute_reply":"2022-04-22T22:17:44.385364Z","shell.execute_reply.started":"2022-04-22T22:17:44.373466Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtst1=Xtst.drop(columns='s5')","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:17:44.853739Z","iopub.status.busy":"2022-04-22T22:17:44.85345Z","iopub.status.idle":"2022-04-22T22:17:44.859984Z","shell.execute_reply":"2022-04-22T22:17:44.859142Z","shell.execute_reply.started":"2022-04-22T22:17:44.853707Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtst1.head()","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:17:45.519478Z","iopub.status.busy":"2022-04-22T22:17:45.518643Z","iopub.status.idle":"2022-04-22T22:17:45.531228Z","shell.execute_reply":"2022-04-22T22:17:45.530447Z","shell.execute_reply.started":"2022-04-22T22:17:45.519441Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtstack.head()","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:17:46.087518Z","iopub.status.busy":"2022-04-22T22:17:46.086812Z","iopub.status.idle":"2022-04-22T22:17:46.101216Z","shell.execute_reply":"2022-04-22T22:17:46.100433Z","shell.execute_reply.started":"2022-04-22T22:17:46.087479Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xts=pd.read_csv('../input/ml-challenge2-kaggle-dataset/x_st.csv')","metadata":{"execution":{"iopub.execute_input":"2022-04-22T22:22:42.864893Z","iopub.status.busy":"2022-04-22T22:22:42.864555Z","iopub.status.idle":"2022-04-22T22:22:42.937599Z","shell.execute_reply":"2022-04-22T22:22:42.936778Z","shell.execute_reply.started":"2022-04-22T22:22:42.864852Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xts.shape","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xts.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using Gradient boosting for stacking ","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"gb400 = GradientBoostingClassifier(max_depth=16,n_estimators=2500,verbose=2)\n\ngb400.fit(Xts,y)\n\n\ny_pred_test_df = gb400.predict(Xtst1)\n\n\nmd_probs = gb400.predict_proba(Xtst1)\n\nb= md_probs[:,1]\n\npr=pd.DataFrame(columns=['High Income'])\npr.head()\n\nfor i in b:\n    print(i)\n    if i>0.30:\n        pr=pr.append({'High Income':1},ignore_index=True)\n    else:\n        pr=pr.append({'High Income':0},ignore_index=True)\n\n\n\n\nXto=pd.read_csv('xts.csv')\nro=Xto['row ID']\ntype(ro)\nrow=ro.values\n\nprd=pd.DataFrame({'row ID' : row, 'High Income': pr['High Income']}, index=None)\nprdb=pd.DataFrame({'row ID' : row, 'High Income': b}, index=None)\n\nprd1=pd.DataFrame({'row ID' : row, 'High Income': y_pred_test_df}, index=None)\n\nprd\nprd.head()\n\nprd.to_csv('o60p.csv',index=False)\nprd1.to_csv('o60y.csv',index=False)\nprdb.to_csv('060.csv',index=False)\n\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using Random Forrest for stacking","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"cell_type":"code","source":"gb400 = RandomForestClassifier(verbose=2, n_estimators=1000, max_depth=50)\n\ngb400.fit(Xts,y)\n\n\n\ny_pred_test_df = gb400.predict(Xtst)\n\n\nmd_probs = gb400.predict_proba(Xtst)\n\nb= md_probs[:,1]\n\npr=pd.DataFrame(columns=['High Income'])\npr.head()\n\nfor i in b:\n    print(i)\n    if i>0.30:\n        pr=pr.append({'High Income':1},ignore_index=True)\n    else:\n        pr=pr.append({'High Income':0},ignore_index=True)\n\n\n\nXto=pd.read_csv('xts.csv')\nro=Xto['row ID']\ntype(ro)\nrow=ro.values\n\nprd=pd.DataFrame({'row ID' : row, 'High Income': pr['High Income']}, index=None)\nprdb=pd.DataFrame({'row ID' : row, 'High Income': b}, index=None)\n\nprd1=pd.DataFrame({'row ID' : row, 'High Income': y_pred_test_df}, index=None)\n\nprd\nprd.head()\n\nprd.to_csv('o61p.csv',index=False)\nprd1.to_csv('o61y.csv',index=False)\nprdb.to_csv('061.csv',index=False)\n\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]}]}